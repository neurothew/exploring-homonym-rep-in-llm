---
title: "analysis_homonymy"
output: rmarkdown::github_document
---

```{r}
# For general purpose and plotting
library(Cairo)
library(tidyverse)
library(arrow)
library(ggpubr)
library(ggsci)
library(patchwork)

# For statistical analysis
library(rstatix)
library(emmeans)
library(ez)
library(xtable)
library(lme4)
library(lmerTest)
library(ggstatsplot)
library(MuMIn)

theme_pubrpaper <- function() {
  # theme_pubr() + 
  theme(
    panel.background = element_rect(fill = "white"),
    legend.background = element_rect(fill = "white"),
    legend.key = element_rect(fill = "transparent", colour = "transparent"),
    legend.position = c(0.9, 0.9),
    panel.grid.minor = element_line(colour = "grey95"),
    panel.grid.major = element_line(colour = "grey95"), 
    panel.ontop = F,
  )
}

fmt_pvalue = function(data, cols = c("p.value", "p.value.adj"), decimals = 3, threshold = 0.001) {
  data %>% 
    gt() %>% 
    fmt_number(
      # columns = all_of(cols),
      decimals = decimals
    ) %>% 
    text_transform(
      locations = cells_body(
        columns = all_of(cols),
        rows = if(length(cols) == 1) {
          get(cols) < threshold
        } else {
          eval(parse(text = paste(cols, "<", threshold, collapse = " | ")))
        }
      ),
      fn = function(x) ifelse(as.numeric(x) < threshold, paste0("<", threshold), x)
    ) %>% 
    as.data.frame()
}


# Define your
wd = ("~/Documents/exploring-homonym-representations-in-llm")
```
# Data preprocessing
This section prepares two dataframe `df_sim_byword` and `df_gap` from the file `df_raw_byword.feather`, which is the dataframe generated by Python scripts.

Columns:
- `cos_sim`: cosine similarity
- `angle_diff`: angular differences (computed as arccos(cos_sim))
- `angle_sim`: 90 - `angle_diff`, has the same interpretation as cosine similarity

Suffix:
`_b`: baseline
`_adj`: adjusted

Sanity check to see if we have really computed all similarity measurements for both same-sense and diff-sense combinations.
```{r}
df_sim_byword = arrow::read_feather(file.path(wd, "statistics//data//df_raw_byword.feather"))
# df_sim_byword %>% select(-cos_sim, -cos_sim_b, -angle_diff_b, -angle_diff, -angle_sim, -angle_sim_b) %>% group_by(model, lang, sense_type) %>% slice(1) %>% ungroup() %>% group_by(model) %>% count() %>% View()
df_sim_byword %>% group_by(model, lang, sense_type) %>% slice(1) %>% ungroup() %>% group_by(model) %>% count() %>% View()
```

Preprocessing
```{r}
df_sim_byword = arrow::read_feather(file.path(wd, "statistics//data//df_raw_byword.feather"))
df_sim_byword$word = factor(df_sim_byword$word)
df_sim_byword$model = factor(df_sim_byword$model)
df_sim_byword$lang = factor(df_sim_byword$lang)
df_sim_byword$sense = factor(df_sim_byword$sense)
df_sim_byword$SamePOS = factor(df_sim_byword$SamePOS)
df_sim_byword$sense_type = factor(df_sim_byword$sense_type)

df_sim_byword = df_sim_byword %>% rename(POS_type = SamePOS,
                                         angle_diff_b = angle_b,
                                         angle_diff = angle)

df_sim_byword$POS_type = factor(df_sim_byword$POS_type, levels=c(0, 1), labels=c("diff", "same"))

df_sim_byword = df_sim_byword %>% select(word, model, layer, layer_rel, lang, POS_type, sense_type, sense, cos_sim, cos_sim_b, angle_diff, angle_diff_b)

df_sim_byword = df_sim_byword %>% mutate(angle_sim = 90 - df_sim_byword$angle_diff)
df_sim_byword = df_sim_byword %>% mutate(angle_sim_b = 90 - df_sim_byword$angle_diff_b)
df_sim_byword = df_sim_byword %>% mutate(angle_sim_adj = angle_sim - angle_sim_b)
df_sim_byword %>% head()

# To save the file
write_feather(df_sim_byword, file.path(wd, "statistics", "data", "df_sim_byword_preprocessed.feather"))

```

Creating the dataframe `df_gap` storing the gap measurement.

`gap`: same-sense angular similarity - cross-sense angular similarity
```{r}
df_gap = df_sim_byword %>% select(-cos_sim, -cos_sim_b, -angle_diff_b, -angle_diff, -angle_sim, -angle_sim_b) %>% pivot_wider(names_from=c("sense_type", "sense"), values_from = "angle_sim_adj") %>% mutate("1" = same_1 - diff_0, "2" = same_2 - diff_0) %>% select(-diff_0, -same_1, -same_2) %>% pivot_longer(c("1", "2"), names_to = "sense", values_to = "gap")

df_gap$sense= factor(df_gap$sense)
df_gap = df_gap %>% mutate(layer_depth=case_when(
  layer_rel <= 0.33 ~ "lower",
  layer_rel > 0.33 & layer_rel <= 0.67 ~ "middle",
  layer_rel > 0.67 ~ "higher"
))
df_gap %>% head()

# To save the file
write_feather(df_gap, file.path(wd, "statistics", "data", "df_gap_preprocessed.feather"))
```

# Generating Figures and Tables in the manuscript

```{r}
df_gap = read_feather(file.path(wd, "statistics", "data", "df_gap_preprocessed.feather"))
```

## Table 1 in the manuscript
```{r}
# The grand average over word, POS_type and sense_type
df_gap_grand_mean = df_gap %>% filter(layer!=0) %>% group_by(model, lang, layer, layer_rel, layer_depth) %>% select(gap) %>% summarize_all(mean)
df_best_gap = df_gap_grand_mean %>% ungroup() %>% group_by(model, lang) %>% arrange(desc(gap)) %>% slice(1)

list_model_levels = c("bert-base-uncased", "bert-large-uncased", "bert-base-chinese", "bert-base-multilingual-uncased", "roberta-base", "roberta-large", "xlm-roberta-base", "xlm-roberta-large", "deberta-v3-base", "deberta-v3-large", "mdeberta-v3-base", "gpt2", "gpt2-medium", "gpt2-large", "gpt2-xl", "Llama-3.2-1B", "Llama-3.2-3B", "Llama-3.1-8B", "Qwen2.5-1.5B", "Qwen2.5-3B", "Qwen2.5-7B")

list_model_en = c("bert-base-uncased", "bert-large-uncased", "bert-base-multilingual-uncased", "roberta-base", "roberta-large", "xlm-roberta-base", "xlm-roberta-large", "deberta-v3-base", "deberta-v3-large", "mdeberta-v3-base", "gpt2", "gpt2-medium", "gpt2-large", "gpt2-xl", "Llama-3.2-1B", "Llama-3.2-3B", "Llama-3.1-8B", "Qwen2.5-1.5B", "Qwen2.5-3B", "Qwen2.5-7B")

list_model_zh = c("bert-base-chinese", "bert-base-multilingual-uncased", "roberta-base", "roberta-large", "xlm-roberta-base", "xlm-roberta-large", "deberta-v3-base", "deberta-v3-large", "mdeberta-v3-base", "gpt2", "gpt2-medium", "gpt2-large", "gpt2-xl", "Llama-3.2-1B", "Llama-3.2-3B", "Llama-3.1-8B", "Qwen2.5-1.5B", "Qwen2.5-3B", "Qwen2.5-7B")

df_best_gap$model = factor(df_best_gap$model, levels=list_model_levels)
df_best_gap$layer_rel = round(df_best_gap$layer_rel*100, 1)

table_1 = df_best_gap
```

Look at the table
```{r}
table_1 %>% head()
```


## Table 4: Testing the main effect of layer per langauge and PLM/LLM
```{r}
# ignore layer 0 which is the embedding layer
df_gap_mean = df_gap %>% filter(layer!=0) %>% group_by(word, lang, model, POS_type, layer, layer_rel) %>% select(gap) %>% summarize_all(mean)
df_gap_mean$POS_type = factor(df_gap_mean$POS_type)
df_gap_mean$layer = factor(df_gap_mean$layer)
df_gap_mean$model = factor(df_gap_mean$model, levels=list_model_levels)
list_model = df_gap_mean$model %>% unique()
list_lang = c("en", "zh")

list_cutpoints = c(0, 0.001, 0.01, 0.05, 1)
list_symbols = c("***", "**", "*", "ns")

# list_cutpoints = c(0, 0.05, 1)
# list_symbols = c("", "x")

emm_options(pbkrtest.limit = 5000)
emm_options(lmerTest.limit = 5000)
compute_layer_main_effect = function(df, this_model, this_lang){
    this_df = df %>% filter(model == this_model, lang == this_lang)
    if (nrow(this_df) == 0){
      return(data.frame()[0, ])
    }
    this_lme = lmer(gap ~ layer + (1|word), this_df)
    # anova_results = anova(lme) %>% as.data.frame() %>% rownames_to_column("Factor")
    emm_layer = emmeans(this_lme, "layer")
    layer_results = joint_tests(emm_layer) %>% as_tibble() %>% add_significance(cutpoints = list_cutpoints, symbols=list_symbols) %>% rename(Factor = "model term")
    # layer_results = anova(this_lme) %>% as.data.frame() %>% rownames_to_column("Factor") %>% add_significance(p.col = "Pr(>F)", cutpoints = list_cutpoints, symbols=list_symbols)
    return (layer_results)
}

# list_model = c("bert-base-chinese")
list_model = df_gap_mean$model %>% unique()
list_lang = c("en", "zh")
layer_main_effects_all = crossing(model = list_model, lang = list_lang) %>% group_by(model, lang) %>% group_modify(~compute_layer_main_effect(df_gap_mean, .y$model, .y$lang))

layer_main_effects_all = layer_main_effects_all %>% ungroup() %>% adjust_pvalue(p.col = "p.value", method="fdr") %>% add_significance(cutpoints=list_cutpoints, symbols=list_symbols)

# Generate the latex table
this_xtable = xtable(layer_main_effects_all %>% fmt_pvalue() %>% mutate(p_latex = paste0("$", p.value.adj, "^{", p.value.adj.signif, "}$")) %>% select(-p.value, -p.value.signif, -p.value.adj, -p.value.adj.signif))

print.xtable(this_xtable, sanitize.text.function = identity, include.rownames = F)

table_4 = layer_main_effects_all
```

Look at the table
```{r}
table_4 %>% head()
```

## Seciont 4.1, 4.2, Appendix C.1
`df_en_model_diff_on_best_dscore.csv` stores the pairwise comparisons between every two models on their best D-scores from English homonym representations.
`df_en_model_diff_on_best_dscore.csv` stores the pairwise comparisons between every two models on their best D-scores computed from Chinese homonym representations.

```{r}
# ignore layer 0 which is the embedding layer
df_gap_mean = df_gap %>% filter(layer!=0) %>% group_by(word, lang, model, POS_type, layer, layer_rel) %>% select(gap) %>% summarize_all(mean)
df_gap_mean$POS_type = factor(df_gap_mean$POS_type)
df_gap_mean$layer = factor(df_gap_mean$layer)
df_gap_mean$model = factor(df_gap_mean$model, levels=list_model_levels)
list_model = df_gap_mean$model %>% unique()
list_lang = c("en", "zh")

# list_cutpoints = c(0, 0.001, 0.01, 0.05, 1)
# list_symbols = c("***", "**", "*", "ns")

list_cutpoints = c(0, 0.05, 1)
list_symbols = c("", "x")

emm_options(pbkrtest.limit = 5000)
emm_options(lmerTest.limit = 5000)

find_best_gap_layer = function(df_gap_mean, df_best_gap, this_model, this_lang){
  best_layer = df_best_gap %>% filter(model==this_model, lang==this_lang) %>% pull(layer)
  
  if (identical(best_layer, numeric(0))){
    return(data.frame()[0, ])
  }
  return(df_gap_mean %>% filter(model==this_model, lang==this_lang, layer==best_layer) %>% ungroup() %>% select(-model, -lang))
}

df_best_gap_all_words = crossing(model = list_model, lang = list_lang) %>% group_by(model, lang) %>% group_modify(~find_best_gap_layer(df_gap_mean, df_best_gap, .y$model, .y$lang))

list_cutpoints = c(0, 0.001, 0.01, 0.05, 1)
list_symbols = c("***", "**", "*", "ns")

lme_best_gap_en = lmer(gap ~ model + (1|word), df_best_gap_all_words %>% filter(lang=="en"))
emm_best_gap_en = emmeans(lme_best_gap_en, "model")
pairs_best_gap_en = pairs(emm_best_gap_en, adjust="fdr") %>% as_tibble()
pairs_best_gap_en = pairs_best_gap_en %>% mutate(modelA = sapply(strsplit(contrast, " - "), `[`, 1), modelB = sapply(strsplit(contrast, " - "), `[`, 2)) %>% select(modelA, modelB, everything()) %>% add_significance(cutpoints = list_cutpoints, symbols=list_symbols)

lme_best_gap_zh = lmer(gap ~ model + (1|word), df_best_gap_all_words %>% filter(lang=="zh"))
emm_best_gap_zh = emmeans(lme_best_gap_zh, "model")
pairs_best_gap_zh = pairs(emm_best_gap_zh, adjust="fdr") %>% as_tibble()
pairs_best_gap_zh = pairs_best_gap_zh %>% mutate(modelA = sapply(strsplit(contrast, " - "), `[`, 1), modelB = sapply(strsplit(contrast, " - "), `[`, 2)) %>% select(modelA, modelB, everything()) %>% add_significance(cutpoints = list_cutpoints, symbols=list_symbols)

write_excel_csv(pairs_best_gap_en, "df_en_model_diff_on_best_dscore.csv")
write_excel_csv(pairs_best_gap_zh, "df_zh_model_diff_on_best_dscore.csv")

```

Look at the table
```{r}
pairs_best_gap_en %>% head()
```

```{r}
pairs_best_gap_zh %>% head()
```

## Table 5: Testing the overall effect of language
Testing the overall effect of language averaged over all layers per model by the pairwise comparisons between the D-scores of the two languages. Multiple comparisons corrected via FDR.
```{r}
# Example
# -------
# lme_lang = lmer(gap ~ lang*layer*POS_type + (1|word), df_gap_mean %>% filter(model=="Llama-3.1-8B"))
# emm_lang = emmeans(lme_lang, "lang")
# joint_tests(emm_lang)
# -------
# ignore layer 0 which is the embedding layer
df_gap_mean = df_gap %>% filter(layer!=0) %>% group_by(word, lang, model, POS_type, layer, layer_rel) %>% select(gap) %>% summarize_all(mean)
df_gap_mean$POS_type = factor(df_gap_mean$POS_type)
df_gap_mean$layer = factor(df_gap_mean$layer)
df_gap_mean$model = factor(df_gap_mean$model, levels=list_model_levels)
list_model = df_gap_mean$model %>% unique()
list_lang = c("en", "zh")

list_cutpoints = c(0, 0.001, 0.01, 0.05, 1)
list_symbols = c("***", "**", "*", "ns")

emm_options(pbkrtest.limit = 10000)
emm_options(lmerTest.limit = 10000)
compute_lang_main_effect = function(df, this_model){
    this_df = df %>% filter(model == this_model)
    if (!this_df %>% pull(lang) %>% unique() %>% length() == 2){
      return(data.frame()[0, ])
    }
    this_lme = lmer(gap ~ lang*layer + (1|word), this_df)
    emm_lang = emmeans(this_lme, "lang")
    lang_results = pairs(emm_lang) %>% as_tibble() %>% add_significance(cutpoints = list_cutpoints, symbols=list_symbols)
    return (lang_results)
}

list_model = df_gap_mean$model %>% unique()
lang_results_all = crossing(model = list_model) %>% group_by(model) %>% group_modify(~compute_lang_main_effect(df_gap_mean, .y$model))
lang_results_all$model = factor(lang_results_all$model, levels=list_model_levels)
lang_results_all = lang_results_all %>% ungroup() %>% adjust_pvalue(p.col = "p.value", method="fdr") %>% add_significance(cutpoints=list_cutpoints, symbols=list_symbols)

# Generate the latex table
this_xtable = xtable(lang_results_all %>% fmt_pvalue() %>% mutate(p_latex = paste0("$", p.value.adj, "^{", p.value.adj.signif, "}$")) %>% select(-p.value, -p.value.signif, -p.value.adj, -p.value.adj.signif))

print.xtable(this_xtable, sanitize.text.function = identity, include.rownames = F)

table_5 = lang_results_all

```

Look at the table
```{r}
table_5 %>% head()
```


## Table 6: The interaction effect between POS and layer

```{r}
# ignore layer 0 which is the embedding layer
df_gap_mean = df_gap %>% filter(layer!=0) %>% group_by(word, lang, model, POS_type, layer, layer_rel) %>% select(gap) %>% summarize_all(mean)
df_gap_mean$POS_type = factor(df_gap_mean$POS_type)
df_gap_mean$layer = factor(df_gap_mean$layer)
df_gap_mean$model = factor(df_gap_mean$model, levels=list_model_levels)
list_model = df_gap_mean$model %>% unique()
list_lang = c("en", "zh")

list_cutpoints = c(0, 0.001, 0.01, 0.05, 1)
list_symbols = c("***", "**", "*", "")

# list_cutpoints = c(0, 0.05, 1)
# list_symbols = c("", "x")

emm_options(pbkrtest.limit = 5000)
emm_options(lmerTest.limit = 5000)
compute_interaction = function(df, this_model, this_lang){
    this_df = df %>% filter(model == this_model, lang == this_lang)
    if (nrow(this_df) == 0){
      return(data.frame()[0, ])
    }
    this_lme = lmer(gap ~ layer*POS_type + (1|word), this_df)
    emm_lme = emmeans(this_lme, ~POS_type | layer)
    anova_results = joint_tests(emm_lme) %>% add_significance(cutpoints = list_cutpoints, symbols=list_symbols)
    # anova_results = anova(this_lme) %>% as.data.frame() %>% rownames_to_column("Factor") %>% add_significance(p.col = "Pr(>F)", cutpoints = list_cutpoints, symbols=list_symbols)
    return (anova_results)
}

# list_model = c("bert-base-chinese")
list_model = df_gap_mean$model %>% unique()
list_lang = c("en", "zh")
anova_results_all = crossing(model = list_model, lang = list_lang) %>% group_by(model, lang) %>% group_modify(~compute_interaction(df_gap_mean, .y$model, .y$lang))

anova_results_all = anova_results_all %>% ungroup() %>% adjust_pvalue(p.col = "p.value", method="fdr") %>% add_significance(cutpoints=list_cutpoints, symbols=list_symbols)

# Generate the latex table
this_xtable = xtable(anova_results_all %>% filter(`model term`=="POS_type:layer") %>% fmt_pvalue() %>% mutate(p_latex = paste0("$", p.value.adj, "^{", p.value.adj.signif, "}$")) %>% select(-p.value, -p.value.signif, -p.value.adj, -p.value.adj.signif))

print.xtable(this_xtable, sanitize.text.function = identity, include.rownames = F)

table_6 = anova_results_all

```

Look at the table
```{r}
table_6 %>% head()
```

## Figure 3: numbers behind 
```{r}
# ignore layer 0 which is the embedding layer
df_gap_mean = df_gap %>% filter(layer!=0) %>% group_by(word, lang, model, POS_type, layer, layer_rel) %>% select(gap) %>% summarize_all(mean)
df_gap_mean$POS_type = factor(df_gap_mean$POS_type)
df_gap_mean$layer = factor(df_gap_mean$layer)
list_model = df_gap_mean$model %>% unique()
list_lang = c("en", "zh")

list_cutpoints = c(0, 0.05, 1)
list_symbols = c("", "x")

emm_options(pbkrtest.limit = 5000)
emm_options(lmerTest.limit = 5000)
compute_posthoc = function(df, this_model, this_lang){
    this_df = df %>% filter(model == this_model, lang == this_lang)
    if (nrow(this_df) == 0){
      return(data.frame()[0, ])
    }
    this_lme = lmer(gap ~ layer*POS_type + (1|word), this_df)
    emm = emmeans(this_lme, "POS_type", by="layer")
    pairs_results = as_tibble(pairs(emm))
    pairs_results = pairs_results %>% adjust_pvalue(method="fdr") %>% add_significance("p.value", cutpoints=list_cutpoints, symbols=list_symbols) %>% add_significance("p.value.adj", cutpoints=list_cutpoints, symbols=list_symbols)
    return (pairs_results)
}

# list_model = c("bert-base-chinese")
list_model = df_gap_mean$model %>% unique()
list_lang = c("en", "zh")
pairs_results_all = crossing(model = list_model, lang = list_lang) %>% group_by(model, lang) %>% group_modify(~compute_posthoc(df_gap_mean, .y$model, .y$lang))

df_layer_info = df_gap_grand_mean %>% select(-gap)
df_layer_info$layer = factor(df_layer_info$layer)

df_pairs_results_all = left_join(pairs_results_all, df_layer_info)
df_pairs_results_all$model = factor(df_pairs_results_all$model, levels=list_model_levels)

# add save here
list_cutpoints = c(0, 0.001, 0.01, 0.05, 1)
list_symbols = c("***", "**", "*", "")
df_pairs_pos_all = df_pairs_results_all %>% select(-p.value.signif, -p.value.adj.signif) %>% add_significance(p.col = "p.value", cutpoints = list_cutpoints, symbols = list_symbols) %>% add_significance(p.col = "p.value.adj", cutpoints = list_cutpoints, symbols = list_symbols)
write_excel_csv(df_pairs_pos_all, "df_pairs_pos_all.csv")

```

## Figure 3: plotting
```{r}

list_model_en = c("bert-base-uncased", "Llama-3.1-8B")
list_model_en = df_pairs_results_all %>% filter(lang=="en") %>% pull(model) %>% unique()
list_model_zh = df_pairs_results_all %>% filter(lang=="zh") %>% pull(model) %>% unique()

list_geom_tile_handle = lapply(list_model_en, create_geom_tile, df=df_pairs_results_all, lang="en")

this_lang = "en"
plot_en_pos = ggplot()
for (this_model in list_model_en){
  plot_en_pos = plot_en_pos + geom_tile(data=df_pairs_results_all %>% filter(model==this_model, lang==this_lang), aes(x=layer_rel, y=model, fill=t.ratio)) + geom_text(data=df_pairs_results_all %>% filter(model==this_model, lang==this_lang), aes(x=layer_rel, y=model, label = p.value.adj.signif))
}
plot_en_pos = plot_en_pos + scale_y_discrete(limits=rev(list_model_levels)) + scale_fill_distiller(palette = "RdYlBu", limits=c(-0.07, 12.83)) + xlab("Relative Layer") + theme_pubclean() + theme(legend.position = "none") + guides(fill = guide_colorbar(barwidth = 8, barheight = 0.5)) + ggtitle("English")


this_lang = "zh"
plot_zh_pos = ggplot()
for (this_model in list_model_zh){
  plot_zh_pos = plot_zh_pos + geom_tile(data=df_pairs_results_all %>% filter(model==this_model, lang==this_lang), aes(x=layer_rel, y=model, fill=t.ratio)) + geom_text(data=df_pairs_results_all %>% filter(model==this_model, lang==this_lang), aes(x=layer_rel, y=model, label = p.value.adj.signif))
}
plot_zh_pos = plot_zh_pos + scale_y_discrete(limits=rev(list_model_levels)) + scale_fill_distiller(palette = "RdYlBu", limits=c(-0.07, 12.83)) + xlab("Relative Layer") + theme_pubclean() + theme(legend.position = "right", axis.text.y = element_blank(), axis.ticks.y=element_blank(), axis.title.y= element_blank()) + guides(fill=guide_colorbar(barwidth=1, barheight=10)) + ggtitle("Chinese")

fig_3 = wrap_plots(plot_en_pos, plot_zh_pos)

```

## Table 8 in the manuscript
Creating the table storing the layer which the POS contributes the most in predicting the gap
```{r}
# English
df_best_pos_en = df_pairs_results_all %>% filter(lang=="en") %>% arrange(desc(t.ratio)) %>% slice(1) %>% ungroup() %>% select(model, layer, t.ratio)
 
df_best_pos_en = left_join(df_best_pos_en, df_layer_info %>% filter(lang=="en") %>% ungroup() %>% select(model, layer, layer_rel, layer_depth)) %>% mutate(layer_rel=round(layer_rel*100, 1))

# Chinese
df_best_pos_zh = df_pairs_results_all %>% filter(lang=="zh") %>% arrange(desc(t.ratio)) %>% slice(1) %>% ungroup() %>% select(model, layer, t.ratio) 

df_best_pos_zh = left_join(df_best_pos_zh, df_layer_info %>% filter(lang=="zh") %>% ungroup() %>% select(model, layer, layer_rel, layer_depth)) %>% mutate(layer_rel=round(layer_rel*100, 1))

df_best_side_by_side_en = left_join(df_best_pos_en %>% ungroup() %>% select(model, layer) %>% rename(layer_best_pos = layer), df_best_gap %>% filter(lang=="en") %>% ungroup() %>% select(model, layer, gap) %>% rename(layer_best_gap = layer))

df_best_side_by_side_zh = left_join(df_best_pos_zh %>% ungroup() %>% select(model, layer) %>% rename(layer_best_pos = layer), df_best_gap %>% filter(lang=="zh") %>% ungroup() %>% select(model, layer, gap) %>% rename(layer_best_gap = layer))

print.xtable(xtable(df_best_side_by_side_en), include.rownames = F)
print.xtable(xtable(df_best_side_by_side_zh), include.rownames = F)

```

Look at the table
```{r}
df_best_side_by_side_en %>% head()
```

```{r}
df_best_side_by_side_zh %>% head()
```


<!-- ## Archive -->
<!-- Computing the correlation -->
<!-- ```{r} -->
<!-- df_for_cor = df_pairs_results_all %>% select(model, lang, layer, t.ratio) -->
<!-- df_for_cor = df_for_cor %>% pivot_wider(names_from=lang, values_from=t.ratio) -->
<!-- df_cor_between_lang = correlation::correlation(df_for_cor, select="zh", select2="en", p_adjust="none") %>% as_tibble() %>% add_significance() %>% adjust_pvalue(p.col="p", method="fdr") %>% add_significance() -->

<!-- df_cor_between_lang %>% select(Group, r, p.adj, p.adj.signif) %>% rename(model=Group) %>% filter(!model %in% c("bert-base-uncased", "bert-large-uncased", "bert-base-chinese")) -->
<!-- ``` -->

